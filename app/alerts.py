"""
å‘Šè­¦ç³»ç»Ÿæ¨¡å—
"""
import asyncio
import json
import smtplib
from datetime import datetime, timedelta
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
from typing import Dict, Any, Optional, List
from enum import Enum

import requests

from .config import settings
from .logging_config import get_logger

logger = get_logger(__name__)


class AlertLevel(str, Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class Alert:
    """å‘Šè­¦å¯¹è±¡"""
    
    def __init__(
        self,
        title: str,
        message: str,
        level: AlertLevel = AlertLevel.WARNING,
        source: str = "SentimentSense",
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.title = title
        self.message = message
        self.level = level
        self.source = source
        self.metadata = metadata or {}
        self.timestamp = datetime.utcnow()
        self.id = f"{self.source}_{self.timestamp.strftime('%Y%m%d_%H%M%S')}"


class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alert_history: List[Alert] = []
        self.alert_cooldown: Dict[str, datetime] = {}
        self.cooldown_period = timedelta(minutes=15)  # 15åˆ†é’Ÿå†·å´æœŸ
    
    def _should_send_alert(self, alert_key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€å‘Šè­¦ï¼ˆé¿å…å‘Šè­¦é£æš´ï¼‰"""
        if alert_key not in self.alert_cooldown:
            return True
        
        last_sent = self.alert_cooldown[alert_key]
        return datetime.utcnow() - last_sent > self.cooldown_period
    
    async def send_alert(self, alert: Alert) -> bool:
        """å‘é€å‘Šè­¦"""
        alert_key = f"{alert.source}_{alert.title}"
        
        if not self._should_send_alert(alert_key):
            logger.debug(f"Alert suppressed due to cooldown: {alert.title}")
            return False
        
        # è®°å½•å‘Šè­¦å†å²
        self.alert_history.append(alert)
        if len(self.alert_history) > 1000:  # ä¿ç•™æœ€è¿‘1000æ¡å‘Šè­¦
            self.alert_history = self.alert_history[-1000:]
        
        success = False
        
        # å‘é€é‚®ä»¶å‘Šè­¦
        if settings.ALERT_EMAIL:
            try:
                await self._send_email_alert(alert)
                success = True
                logger.info(f"Email alert sent: {alert.title}")
            except Exception as e:
                logger.error(f"Failed to send email alert: {e}")
        
        # å‘é€ Slack å‘Šè­¦
        if settings.ALERT_SLACK_WEBHOOK:
            try:
                await self._send_slack_alert(alert)
                success = True
                logger.info(f"Slack alert sent: {alert.title}")
            except Exception as e:
                logger.error(f"Failed to send Slack alert: {e}")
        
        if success:
            self.alert_cooldown[alert_key] = datetime.utcnow()
        
        return success
    
    async def _send_email_alert(self, alert: Alert):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…ç”Ÿäº§ç¯å¢ƒéœ€è¦é…ç½® SMTP æœåŠ¡å™¨
        logger.info(f"Would send email alert: {alert.title} - {alert.message}")
    
    async def _send_slack_alert(self, alert: Alert):
        """å‘é€ Slack å‘Šè­¦"""
        if not settings.ALERT_SLACK_WEBHOOK:
            return
        
        # æ ¹æ®å‘Šè­¦çº§åˆ«é€‰æ‹©é¢œè‰²
        color_map = {
            AlertLevel.INFO: "#36a64f",      # ç»¿è‰²
            AlertLevel.WARNING: "#ff9500",   # æ©™è‰²
            AlertLevel.ERROR: "#ff0000",     # çº¢è‰²
            AlertLevel.CRITICAL: "#8b0000"   # æ·±çº¢è‰²
        }
        
        payload = {
            "attachments": [
                {
                    "color": color_map.get(alert.level, "#ff9500"),
                    "title": f"ğŸš¨ {alert.title}",
                    "text": alert.message,
                    "fields": [
                        {
                            "title": "Service",
                            "value": alert.source,
                            "short": True
                        },
                        {
                            "title": "Level",
                            "value": alert.level.upper(),
                            "short": True
                        },
                        {
                            "title": "Time",
                            "value": alert.timestamp.strftime("%Y-%m-%d %H:%M:%S UTC"),
                            "short": True
                        }
                    ]
                }
            ]
        }
        
        # æ·»åŠ å…ƒæ•°æ®
        if alert.metadata:
            for key, value in alert.metadata.items():
                payload["attachments"][0]["fields"].append({
                    "title": key.replace("_", " ").title(),
                    "value": str(value),
                    "short": True
                })
        
        # å‘é€åˆ° Slack
        response = requests.post(
            settings.ALERT_SLACK_WEBHOOK,
            json=payload,
            timeout=10
        )
        response.raise_for_status()
    
    def get_recent_alerts(self, hours: int = 24) -> List[Alert]:
        """è·å–æœ€è¿‘çš„å‘Šè­¦"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        return [
            alert for alert in self.alert_history
            if alert.timestamp > cutoff_time
        ]


# å…¨å±€å‘Šè­¦ç®¡ç†å™¨
alert_manager = AlertManager()


# ä¾¿æ·å‡½æ•°
async def send_info_alert(title: str, message: str, **metadata):
    """å‘é€ä¿¡æ¯çº§åˆ«å‘Šè­¦"""
    alert = Alert(title, message, AlertLevel.INFO, metadata=metadata)
    return await alert_manager.send_alert(alert)


async def send_warning_alert(title: str, message: str, **metadata):
    """å‘é€è­¦å‘Šçº§åˆ«å‘Šè­¦"""
    alert = Alert(title, message, AlertLevel.WARNING, metadata=metadata)
    return await alert_manager.send_alert(alert)


async def send_error_alert(title: str, message: str, **metadata):
    """å‘é€é”™è¯¯çº§åˆ«å‘Šè­¦"""
    alert = Alert(title, message, AlertLevel.ERROR, metadata=metadata)
    return await alert_manager.send_alert(alert)


async def send_critical_alert(title: str, message: str, **metadata):
    """å‘é€ä¸¥é‡çº§åˆ«å‘Šè­¦"""
    alert = Alert(title, message, AlertLevel.CRITICAL, metadata=metadata)
    return await alert_manager.send_alert(alert)


class HealthMonitor:
    """å¥åº·ç›‘æ§å™¨"""
    
    def __init__(self):
        self.last_health_status = None
        self.consecutive_failures = 0
        self.max_failures = 3
    
    async def check_and_alert(self, health_result):
        """æ£€æŸ¥å¥åº·çŠ¶æ€å¹¶å‘é€å‘Šè­¦"""
        current_status = health_result.status
        
        # çŠ¶æ€å˜åŒ–å‘Šè­¦
        if self.last_health_status and self.last_health_status != current_status:
            if current_status == "unhealthy":
                await send_error_alert(
                    "Service Health Degraded",
                    f"Service status changed from {self.last_health_status} to {current_status}",
                    previous_status=self.last_health_status,
                    current_status=current_status,
                    uptime=health_result.uptime
                )
            elif current_status == "healthy" and self.last_health_status == "unhealthy":
                await send_info_alert(
                    "Service Health Recovered",
                    f"Service status recovered from {self.last_health_status} to {current_status}",
                    previous_status=self.last_health_status,
                    current_status=current_status,
                    uptime=health_result.uptime
                )
        
        # è¿ç»­å¤±è´¥å‘Šè­¦
        if current_status == "unhealthy":
            self.consecutive_failures += 1
            if self.consecutive_failures >= self.max_failures:
                await send_critical_alert(
                    "Service Continuously Unhealthy",
                    f"Service has been unhealthy for {self.consecutive_failures} consecutive checks",
                    consecutive_failures=self.consecutive_failures,
                    uptime=health_result.uptime
                )
        else:
            self.consecutive_failures = 0
        
        # ç»„ä»¶ç‰¹å®šå‘Šè­¦
        for component in health_result.components:
            if component.status == "unhealthy":
                await send_warning_alert(
                    f"Component {component.name} Unhealthy",
                    component.message or f"Component {component.name} is not healthy",
                    component=component.name,
                    component_status=component.status,
                    response_time=component.response_time
                )
        
        self.last_health_status = current_status


# å…¨å±€å¥åº·ç›‘æ§å™¨
health_monitor = HealthMonitor()
